## Compiler

### [Building a simple JIT in Rust](http://www.jonathanturner.org/2015/12/building-a-simple-jit-in-rust.html)

* Allocate aligned memory
* Enable execution
* Just to be safe, fill it with RET(0xc3)
* Rust-specific
  * `transmute` the memory from `void *` to `mut *u8`
  * Add indexing functions
* Use an assembler to convert your instructions into raw hex, then write it into the executable memory
* Rust-specific
  * `transmute` the executable memory to a function
  * Call this function

#### Debugging

* Use LLDB
* Set a breakpoint on the line calling the JIT code
* Run the program, read the memory behind the function
* Look if the raw hex is correctly written. You can use the disassembler.


So...this is like...shell code?


## Architecture

### [Why is one loop so much slower than two loops?](http://stackoverflow.com/questions/8547778/why-is-one-loop-so-much-slower-than-two-loops)

* Alignment & false aliasing
* Cache bank conflicts
* L1 <-> L2 cache bandwidth(when the data doesn't fit in L1 cache)
* Separating the operation in two loops makes the stride in memory smaller, so it would be much easier for the processor cache to keep up with the memory demand, and make correct predictions
* [What is “cache-friendly” code?](http://stackoverflow.com/questions/16699247/what-is-cache-friendly-code)
* [Loop tiling](https://en.wikipedia.org/wiki/Loop_tiling)

### [Performance of breaking apart one loop into two loops](http://stackoverflow.com/questions/9634754/performance-of-breaking-apart-one-loop-into-two-loops)

* Cons: If your data doesn't fint into the L1 cache, you will have to reload it in the second loop
* Cons: more generated code
* 2x branch prediction
* Pro: reduce register/stack pressure
* Pro: smaller memory stride
* Pro: if the code in the loop will trash the L1 cache anyway...

### [C/C++ tip: How to loop through multi-dimensional arrays quickly](http://nadeausoftware.com/articles/2012/06/c_c_tip_how_loop_through_multi_dimensional_arrays_quickly)

> High-performance code instead implements a multi-dimensional array as a single linear array with hand-authored array indexing math to keep track of what values are where

```
value = data[ i * height * depth + j * depth + k ];
```

> Since the array is a single large chunk of memory, sweeping through it from start-to-finish creates a regular access pattern that processor prefetchers easily recognize, which enables them to load caches in the background. The result is fewer cache misses and much better performance.

#### Classic case: matrix multiplication

* [Visualization: matrix multiply with cache blocking](https://www.youtube.com/watch?v=IFWgwGMMrh0)
* [How to Use Loop Blocking to Optimize Memory Use on 32-Bit Intel® Architecture](https://software.intel.com/en-us/articles/how-to-use-loop-blocking-to-optimize-memory-use-on-32-bit-intel-architecture)
* [Loop nest optimization](https://en.wikipedia.org/wiki/Loop_nest_optimization)
* [Loop interchange](https://en.wikipedia.org/wiki/Loop_interchange)
