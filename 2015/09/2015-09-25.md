## OS

### [Concurrency is not parallelism](https://vimeo.com/49718712)

[Slides](https://talks.golang.org/2012/waza.slide)

* Concurrency is the composition of independently executed things -- typically, functions
  * Dealing with a lot of things at once
  * It is about **structure**
* Parallelism is the simultaneous execution of multiple things
  * Doing a lot of things at once
  * It is about **execution**
* Analogy
  * Concurrent: keyboard, mouse, etc. Not necessarily running in parallel
  * Parallel: vector dot product
* Communication
  * You need communication to coordinate between the independent executions
  * Communicating sequential processes

#### Metaphor -- gophers and books


* The point is to have managable pieces of execution, each may belongs to a certain stage, so you can get more throughput
* Maybe I can do an animated gopher for this, LOL
* You don't necessarily run all the pieces simultaneously, but the design is still correct even if only one piece can be executed at the time. The user don't have to worry about if they had done parallelism right.
* Once you have breakdown the pieces of execution, they can be arranged in different ways to improve the performance

#### Go

* Goroutines
  * Functions running independently in the same address space as other goroutines
  * Much cheaper than threads. You can create thousands of them
  * Multiplexed onto OS threads
  * When a goroutine blocks, that thread blocks but no other goroutine blocks
  * `go func()` like putting `&` in shell commands
* Channels
  * Typed values for goroutines to synchronize and exchange information
  * example

    ```go
    timerChan := make(chan time.Time)
    go func() {
        time.Sleep(deltaT)
        timerChan <- time.Now() // send time on timerChan
    }()
    // Do something else; when ready, receive.
    // Receive will block until timerChan delivers.
    // Value sent is other goroutine's completion time.
    completedAt := <-timerChan
    ```
* Select
  * Like `switch` but the decision is based on **the ability of channels to communicate**
  * If there's no `default`, the system will wait until one of the channel is ready
  * If more than one of the channels are ready, the system will select one of them randomly
  * example

    ```go
    select {
    case v := <-ch1:
        fmt.Println("channel 1 sends", v)
    case v := <-ch2:
        fmt.Println("channel 2 sends", v)
    default: // optional
        fmt.Println("neither channel was ready")
    }
    ```
* Closures

  ```go
  func Compose(f, g func(x float) float)
                    func(x float) float {
       return func(x float) float {
          return f(g(x))
      }
  }

  print(Compose(sin, cos)(0.5))
  ```


#### A simple load balancer

```go
type Work struct {
    x, y, z int
}

// other workers can run when one blocks
func worker(in <-chan *Work, out chan<- *Work) {
   for w := range in {
      w.z = w.x * w.y
      Sleep(w.z)
      out <- w
   }
}

func Run() {
   in, out := make(chan *Work), make(chan *Work)
   for i := 0; i < NumWorkers; i++ {
       go worker(in, out)
   }
   go sendLotsOfWork(in)
   receiveLotsOfResults(out)
}
```

#### Why is Go a good choice for this

* The execution of the program is **implicitly parallel and scalable**
* No explicit synchronization needed. The structure of the program is **implicitly synchronized**

#### Another load balancer

```go
type Request struct {
    fn func() int  // The operation to perform.
    c  chan int    // The channel to return the result.
}

// The requester sends Requests to the balancer
func requester(work chan<- Request) {
    c := make(chan int)
    for {
        // Kill some time (fake load).
        Sleep(rand.Int63n(nWorker * 2 * Second))
        work <- Request{workFn, c} // send request
        result := <-c              // wait for answer
        furtherProcess(result)  
    }    
}

type Worker struct {
    requests chan Request // work to do (buffered channel)
    pending  int          // count of pending tasks
    index     int         // index in the heap
}

// note each response goes directly to its requester
// you could run the loop body as a goroutine for parallelism
func (w *Worker) work(done chan *Worker) {
    for {
        req := <-w.requests // get Request from balancer
        req.c <- req.fn()   // call fn and send result
        done <- w           // we've finished this request
    }
}

type Pool []*Worker

type Balancer struct {
    pool Pool
    done chan *Worker
}

func (b *Balancer) balance(work chan Request) {
    for {
        select {
        case req := <-work: // received a Request...
            b.dispatch(req) // ...so send it to a Worker
        case w := <-b.done: // a worker has finished ...
            b.completed(w)  // ...so update its info
        }
    }
}

// Make Pool an implementation of the Heap interface
// so we can balance by making the Pool a heap tracked by load
func (p Pool) Less(i, j int) bool {
    return p[i].pending < p[j].pending
}

// Send Request to worker
func (b *Balancer) dispatch(req Request) {
    // Grab the least loaded worker...
    w := heap.Pop(&b.pool).(*Worker)
    // ...send it the task.
    w.requests <- req
    // One more in its work queue.
    w.pending++
    // Put it into its place on the heap.
    heap.Push(&b.pool, w)
}

// Job is complete; update heap
func (b *Balancer) completed(w *Worker) {
    // One fewer in the queue.
    w.pending--
    // Remove it from heap.                  
    heap.Remove(&b.pool, w.index)
    // Put it into its place on the heap.
    heap.Push(&b.pool, w)
}
```

#### Another example: Query a replicated database

```go
func Query(conns []Conn, query string) Result {
    ch := make(chan Result, len(conns))  // buffered
    for _, conn := range conns {
        go func(c Conn) {
            ch <- c.DoQuery(query):
        }(conn)
    }
    return <-ch
}
```